---
title: "PSTAT131 HW4"
author: "Joshua Price"
date: "5/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, include = FALSE}
library(tidymodels)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
library(pROC)
library(tidyverse)
tidymodels_prefer()


```

```{r echo=FALSE, include = FALSE}
titanic <- read.csv("titanic.csv")
titanic$survived <- as.factor(titanic$survived)
titanic$survived <- factor(titanic$survived, levels = c("Yes", "No"))
titanic$pclass <- as.factor(titanic$pclass)
set.seed(2505)
```







**Question 1**  
Split the data, stratifying on the outcome variable, survived. You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations.

```{r}
titanic_split <- initial_split(titanic, prop=.7, strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
dim(titanic_train)
dim(titanic_test)
```
Recipe
```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare,data=titanic_train) %>% step_impute_linear(age) %>% step_dummy(all_nominal_predictors()) %>% step_interact(terms=~starts_with("sex"):fare +age:fare)
```
 
**Question 2**  
Fold the training data. Use k-fold cross-validation, with k=10.


```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds

```


**Question 3**  
In your own words, explain what we are doing in Question 2. What is k-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we did use the entire training set, what resampling method would that be?  
-We are splitting the training data into 10 groups that are helping train and test the model. K-folds helps prevent overfitting. If we just used the entire training set, then that would be the validation set approach




**Question 4**  
Set up workflows for 3 models:

A logistic regression with the glm engine;
A linear discriminant analysis with the MASS engine;
A quadratic discriminant analysis with the MASS engine.
How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you’ll fit to each fold.  
-3 models to 10 folds so 30 total model fittings


```{r}
log_reg <- logistic_reg() %>% set_engine("glm") %>% set_mode("classification")

log_wkflow <- workflow() %>% add_model(log_reg) %>% add_recipe(titanic_recipe)


lda_mod <- discrim_linear() %>% set_mode("classification") %>% set_engine("MASS")

lda_wkflow <- workflow() %>% add_model(lda_mod) %>% add_recipe(titanic_recipe)



qda_mod <- discrim_quad() %>% set_mode("classification") %>% set_engine("MASS")

qda_wkflow <- workflow() %>% add_model(qda_mod) %>% add_recipe(titanic_recipe)


```


**Question 5**  
Fit each of the models created in Question 4 to the folded data.

IMPORTANT: Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of loading and saving. You should still include the code to run them when you knit, but set eval = FALSE in the code chunks.

```{r}
log_fit_rs <- 
log_wkflow %>% 
fit_resamples(titanic_folds)
log_fit_rs
```

```{r}
lda_fit_rs <- 
lda_wkflow %>% 
fit_resamples(titanic_folds)
lda_fit_rs
```

```{r}
qda_fit_rs <-
qda_wkflow %>% 
fit_resamples(titanic_folds)

qda_fit_rs
```



**Question 6**  
Use collect_metrics() to print the mean and standard errors of the performance metric accuracy across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. (Note: You should consider both the mean accuracy and its standard error.)  
-The logistic regression model had the highest accuracy and 2nd lowest standard error, though the error is very close between the 3.


```{r}
collect_metrics(log_fit_rs)
collect_metrics(lda_fit_rs)
collect_metrics(qda_fit_rs)
```

**Question 7**  
Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
log_fit <- fit(log_wkflow, titanic_train)

```

**Question 8**  
Finally, with your fitted model, use predict(), bind_cols(), and accuracy() to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.  
-The testing data fit is slightly more accurate than the folds accuracy
```{r}
log_pred <- predict(log_fit,new_data=titanic_test %>%select(-survived))
log_pred <- bind_cols(log_pred,titanic_test %>% select(survived))
log_pred

log_reg_acc <- augment(log_fit, new_data=titanic_test) %>% accuracy(truth=survived,estimate=.pred_class)
log_reg_acc
```


